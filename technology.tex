 \subsection{Implementation technologies}
We plan to use Javascript and Ruby on Rails to implement a simple interface that allows us to present users with easy to understand tasks and collect data from the users both while they complete the tasks and then of their impressions of the tasks after the tasks are over. By combining different incentives with different tasks, we hope to make some conclusions about the space of incentive-task pairings and see whether or not some incentives are more effective when paired with specific tasks than others. When the user arrives at our site, they will be randomly assigned incentive-task pairings, and after they complete the task, they will be asked to evaluate their experience (See: User Evaluations).
 
 \subsection{Incentive implementation}

\subsubsection{Money}
Because our system will not necessarily run on the Mechanical Turk platform, monetary incentives will take the form of Amazon giftcard money. When a task is first presented to the user, an alert displayed via client-side JavaScript will inform them that successful completion of the task will enable them to receive "\$X" amount of giftcard money via a redemption code. This alert will then recede into an unobtrusive position on the screen. The user will be provided with their redemption code after clicking "next" and before the display of the task survey.

In an alternative condition, users will be told that this money will be donated to the charity "Doctors Without Borders" in the event of successful task completion.
\subsubsection{Badges} 
Another incentive will be to associate small achievements within each task with badges that a user can accumulate throughout the duration of the task. A notification will display after the user has earned a badge, and badges received will be displayed in an unobtrusive position on the screen. By being told of other badges they have yet to earn, the user will be motivated to continue.
\subsubsection{Leveling up} 
A leveling-up incentive is similar to a badge in that it reflects achievements throughout the task. However, after leveling-up, a user might unlock additional abilities. Additionally, elements of the task might change; for example, the task might increase in speed, making game play more difficult. Instead of accruing a variety of badges, the user will only have one level: Beginner, Novice, Expert, etc. 
\subsubsection{Social comparison}
The final incentive consists of a social comparison or "leaderboard" system that informs the user of his or her rank within the task relative to peers. During a fixed temporal point within each task, an unobtrusive alert displayed via client-side JavaScript will begin to inform the user that "X" others have reached the same level of success, that "Y" have done worse, and that "Z" have done better ("X", "Y", and "Z" all being artificially generated numbers whose ratios to each other will be hard-coded but whose actual amounts will vary). Subsequent fixed temporal points will cause these amounts to change appropriately (but, once again, regardless of actual numerical truth). When the user clicks next to proceed to the next task or finish the session, the final status of this "leaderboard" will briefly be displayed to the user before the task survey.
 
 \subsection{Task implementation}
\subsubsection{Rhythm game}
Many games employ scrolling pane of instructions, usually four or five, which correlate to buttons the user presses at the correct moment (when the instruction passes a certain point on the screen.) Popular games of this type, sometimes called "rhythm games," include Dance Dance Revolution, Guitar Hero and Rock Band. Because many people are already accustomed to this type of task, we expect the task to have a "high" value of inherent interest and hope that users will be able to play it without requiring a training period. To integrate the game with our incentive structure, we will correlate badges to "streaks" ("x" correct in a row), leveling up to increasing the speed of the instructions, and money and social comparison to a score based on the percentage of instructions correctly followed.
\subsubsection{Text-based game}
The second task that we expect to have relatively "high" values of inherent interest is a text-based adventure game in the vein of classical command-line based interactive fiction. Users will be eased into the game via initial "suggested moves" written into the console to ensure their general comfort with this style of gameplay. The game itself is a standard hero quest involving exploration, discovery, and battles that will be implemented entirely from scratch in JavaScript to ensure proper integration with our incentive structure.

The deepest tree in the interactive fiction is expected to contain 35 steps total, with an estimated completion time of 15 minutes. The shortest tree in the game is expected to contain 10 steps total, with an estimated completion time of less than one minute. Users will be able to restart after in-game "death" and the option to reset the game to its original state will persist throughout gameplay. 
\subsubsection{Clicking on targets} 
A simple target-clicking task, similar to those used for testing Fitt's Law, is not expected to generate much inherent interest, but can be incentivized in similar ways to the rhythm game task. As the user clicks randomly-placed dots of different sizes on screen (drawn using JavaScript and HTML5 Canvas), money and social comparison can be evaluated using percentage of total correct clicks, while badges can be based on streaks of correct clicks and leveling-up can increase the rate of new targets shown on screen.
 
\subsubsection{Typing speed/accuracy} 
The task that we expect to generate "moderate" amounts of inherent interest is a JavaScript-based typing speed test of the type common in both introductory computer skills classes and online applets. Users will be presented with several paragraphs worth of text taken from public domain sources and asked to type their content out to the best of their ability.
 \subsection{Database entries}
Our data will be stored on two separate standard MySQL databases. The first database will correspond to data derived from the potency and interest evaluation portions of our experiment, in which users will evaluate single tasks and incentive schemes on the basis of their inherent interest and incentive potency values. The data from this stage will be structured as follows: a session hash corresponding to a single user's activity, a trial number denoting a trial within a user's session, the ID of the incentive given on each trial (with the task held constant), the potency value assigned by the user to each incentive within a trial, the preference order assigned by the user to each incentive within a trial, the ID of the task given on each trial (with the incentive held constant), the interest value assigned by the user to each task within a trial, the task difficulty assigned by the user to each task within a trial, and, finally, the preference order assigned by the user to each task within a trial.

The second database will correspond to data derived from the actual engagement evaluation portion of our experiment. Users in this subsection will be assigned to two of the four tasks in pseudorandom order (to ensure that tasks are not duplicated) and to one of the incentive conditions. The data from this stage will differ from that in the previous phase in the following ways: an additional field will be associated with each trial indicating how much additional time a user spent on a task after the "next" button appeared and the potency value for each incentive type as well as the inherent interest value for each task type will be a fixed value derived from the first portion of experimentation. Furthermore, fields will be added to represent data derived from the user survey at the end of each session, including records of responses to the general demographics, task experience, and interest level questionnaire and responses for evaluations of the difficulty, self-efficacy, engagement, and replay value for each task type.

\subsection{User evaluations}
After completing the tasks, the user will be asked several questions that we will use to gauge their interest level and the effect of the incentives on their experiences. They will be asked which incentive they preferred, as well as asked to rate task difficulty, efficacy, interest, and which task they preferred. Before the tasks begin, general interest questions will help us to gauge their experience level with particular kinds of games and see what kind of gaming "style" they might have. The most interesting component of our user evaluations is the "Give me more time" button. For each task, the user will be given the option of playing it for longer than we require for our evaluation. The percentage of users who select "Give me more time" will help us determine how engaging a task is. This is a more accurate means of determining how much a user likes a task than asking them, for example, "Would you play this game again?" since many users might answer yes out of politeness, while playing the game for longer than they are required is a sure sign that they enjoy it. We might also provide them with a bookmark and measure what percentage of users bookmark the task to return to and play again.