\section{Implementation technologies}

We use Javascript and PHP to implement a simple interface that allows us to present users with easy to understand tasks and collect data from the users both while they complete the tasks and then of their impressions of the tasks after the tasks are over. By combining different incentives with different tasks, we hope to make some conclusions about the space of incentive-task pairings and see whether or not some incentives are more effective when paired with specific tasks than others. When the user arrives at our site, they will be randomly assigned incentive-task pairings, and after they complete the task, they will be asked to evaluate their experience (See: User Evaluations).
\subsection{Incentive implementation}

\subsubsection{Leveling up}
A leveling-up incentive is similar to a badge in that it reflects achievements throughout the task. Additionally, elements of the task might change; for example, the task might increase in speed, making game play more difficult. Instead of accruing a variety of badges, the user will only have one level: Beginner, Novice, Expert, etc.

\subsubsection{Badges}
Another incentive will be to associate small achievements within each task with badges that a user can accumulate throughout the duration of the task. A notification will display after the user has earned a badge, and badges received will be displayed in an unobtrusive position on the screen. By being told of other badges they have yet to earn, the user will be motivated to continue.

\subsubsection{Social comparison}
The final incentive consists of a social comparison or "leaderboard" system that informs the user of his or her rank within the task relative to peers. During a fixed temporal point within each task, an unobtrusive alert displayed via client-side JavaScript will begin to inform the user that "You're currently better than X\% of other players!" ("X" being artificially generated and unrelated to actual performance). When the user clicks next to proceed to the next task or finish the session, the final status of this "leaderboard" will briefly be displayed to the user before the task survey.

\subsection{Task implementation}
\subsubsection{Text-based adventure game}
The second task that we expect to have relatively "high" values of inherent interest is a text-based adventure game in the vein of classical command-line based interactive fiction. Users will be eased into the game via initial "suggested moves" written into the console to ensure their general comfort with this style of gameplay. The game itself is a standard hero quest involving exploration, discovery, and battles that will be implemented entirely from scratch in JavaScript to ensure proper integration with our incentive structure.

The deepest tree in the interactive fiction is expected to contain 35 steps total, with an estimated completion time of 15 minutes. The shortest tree in the game is expected to contain 10 steps total, with an estimated completion time of less than one minute. Users will be able to restart after in-game "death" and the option to reset the game to its original state will persist throughout gameplay.
\subsubsection{Word bounce}
A simple target-clicking task, similar to those used for testing Fitt's Law, is not expected to generate much inherent interest, but can be incentivized in similar ways to the rhythm game task. As the user clicks randomly-placed dots of different sizes on screen (drawn using JavaScript and HTML5 Canvas), social comparison can be evaluated using percentage of total correct clicks, while badges can be based on streaks of correct clicks and leveling-up can increase the rate of new targets shown on screen.
\subsubsection{Word scramble}
The task that we expect to generate "moderate" amounts of inherent interest is a JavaScript-based "word scramble" in which a sequence of letters is presented and the user must create as many real words as possible from this subset within a set time limit.
\subsection{Database entries}
Our data is stored in a MySQL database, and is structured as follows: a session hash corresponding to a single user's activity, a trial number denoting a trial within a user's session, the ID of the incentive given on each trial (with the task held constant), the potency value assigned by the user to each incentive within a trial, the preference order assigned by the user to each incentive within a trial, a field will be associated with each trial indicating how much additional time a user spent on a task after the "next" button appeared, the ID of the task given on each trial (with the incentive held constant), the interest value assigned by the user to each task within a trial, the task difficulty assigned by the user to each task within a trial, and, finally, the preference order assigned by the user to each task within a trial.

The second database will correspond to data derived from the actual engagement evaluation portion of our experiment. Users in this subsection will be assigned to two of the four tasks in pseudorandom order (to ensure that tasks are not duplicated) and to one of the incentive conditions. The data from this stage will differ from that in the previous phase in the following ways: an additional and the potency value for each incentive type as well as the inherent interest value for each task type will be a fixed value derived from the first portion of experimentation. Furthermore, fields will be added to represent data derived from the user survey at the end of each session, including records of responses to the general demographics, task experience, and interest level questionnaire and responses for evaluations of the difficulty, self-efficacy, engagement, and replay value for each task type.

\subsection{User evaluations}
After completing each task, the user will be asked several questions that we will use to gauge their interest level and the effect of the incentives on their experiences. They will be asked to rate, on a five-point scale, how much they enjoyed playing the previous game, how challenging they found it how often they play similar games, how much they generally enjoy games of a similar type, how likely they would be to replay the previous game in the future, and, if an incentive was present, how motivating the presence of the incentive was.

These questions are intended to measure the subjective feelings of each participant towards the tasks, and to facilitate segmenting our audience into “types”. The questions about the frequency of playing and enjoying similar games give us insight into the background that a particular user might have that might influence his or her responses and engagement levels, as well as providing a basis on which to analyze data for experience/expertise effects. 
